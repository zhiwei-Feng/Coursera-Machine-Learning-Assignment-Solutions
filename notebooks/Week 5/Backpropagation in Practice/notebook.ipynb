{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__Implementation Note: Unrolling Parameters__\n",
    "\n",
    "In order to use optimizing functions such as \"fminunc()\", we will want to \"unroll\" all the elements and put them into one long vector:\n",
    "\n",
    "![summarize](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Checking__\n",
    "\n",
    "We can approximate the derivative of our cost function with:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\Theta} J(\\Theta) \\approx \\frac{J(\\Theta+\\epsilon)-J(\\Theta-\\epsilon)}{2 \\epsilon}$\n",
    "\n",
    "With multiple theta matrices, we can approximate the derivative with respect to $\\Theta_j$ , as follows:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\Theta_{j}} J(\\Theta) \\approx \\frac{J\\left(\\Theta_{1}, \\ldots, \\Theta_{j}+\\epsilon, \\ldots, \\Theta_{n}\\right)-J\\left(\\Theta_{1}, \\ldots, \\Theta_{j}-\\epsilon, \\ldots, \\Theta_{n}\\right)}{2 \\epsilon}$\n",
    "\n",
    "In octave we can do it as follows:\n",
    "\n",
    "```\n",
    "epsilon = 1e-4;\n",
    "for i = 1:n,\n",
    "  thetaPlus = theta;\n",
    "  thetaPlus(i) += epsilon;\n",
    "  thetaMinus = theta;\n",
    "  thetaMinus(i) -= epsilon;\n",
    "  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)\n",
    "end;\n",
    "```\n",
    "\n",
    "So once we compute our gradApprox vector, we can check that gradApprox ≈ deltaVector.\n",
    "\n",
    "Once you have verified once that your backpropagation algorithm is correct, you don't need to compute gradApprox again.\n",
    "_Please turn off the gradient checking._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Initialization__\n",
    "\n",
    "Initializing all theta weights to zero does not work with neural networks.\n",
    "\n",
    "Hence, we initialize each Θ(l)ij to a random value between[−ϵ,ϵ]. Using the above formula guarantees that we get the desired bound.\n",
    "\n",
    "```\n",
    "If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.\n",
    "\n",
    "Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "```\n",
    "\n",
    "(Note: the epsilon used above is unrelated to the epsilon from Gradient Checking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Putting it Together__\n",
    "\n",
    "First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers in total you want to have.\n",
    "\n",
    "- Number of input units = dimension of features $x^{(i)}$\n",
    "- Number of output units = number of classes\n",
    "- Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)\n",
    "- Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.\n",
    "\n",
    "_Training a Neural Network_\n",
    "1. Randomly initialize the weights\n",
    "2. Implement forward propagation to get $h_{\\Theta}\\left(x^{(i)}\\right)$ for any $x^{(i)}$\n",
    "3. Implement the cost function\n",
    "4. Implement backpropagation to compute partial derivatives\n",
    "5. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.\n",
    "6. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
